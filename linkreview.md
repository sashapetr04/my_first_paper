| Название | Год | Автор | Ссылка | Краткое содержание |
| -------- |---- | ----- | ------ | ---- |
|Zero-Shot Recommendation AI Models for Efficient Job–Candidate Matching in Recruitment Process|2024|Jarosław Kurek, Tomasz Latkowski, Michał Bukowski, Bartosz Świderski, Mateusz Łępicki, Grzegorz Baranik, Bogusz Nowak, Robert Zakowicz, Łukasz Dobrakowski|[link](https://www.mdpi.com/2076-3417/14/6/2601)|-|
|Semantic Approaches Survey for Job Recommender Systems|2022|Assia Brek, Zizette Boufaida|[link](https://www.researchgate.net/publication/362482440_Semantic_Approaches_Survey_for_Job_Recommender_Systems)|-|
|Survey on Job Recommendation Systems using Machine Learning|2023|Raj Thali, Suyog Mayekar, Shubham More|[link](https://www.researchgate.net/publication/370176981_Survey_on_Job_Recommendation_Systems_using_Machine_Learning)|-|
|Data Mining Approach to Job Recommendation Systems|2021|Tanvi Tayade, Rutuja Akarte, Gayatree Sorte, Rohit Tayade|[link](https://www.researchgate.net/publication/347554505_Data_Mining_Approach_to_Job_Recommendation_Systems)|-|
|Attention Is All You Need|2017|Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin|[link](https://arxiv.org/abs/1706.03762)|-|
|BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding|2018|Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova|[link](https://arxiv.org/abs/1810.04805)|-|
|MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers|2022|Kaitao Song, Yichong Leng, Xu Tan, Yicheng Zou, Tao Qin, Dongsheng Li|[link](https://arxiv.org/abs/2205.12986)|-|
|Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks|2019|Nils Reimers, Iryna Gurevych|[link](https://arxiv.org/abs/1908.10084)|-|
|Generative Pre-trained Transformer: A Comprehensive Review on Enabling Technologies, Potential Applications, Emerging Challenges, and Future Directions|2024|Gokul Yenduri, Ramalingam M, Chemmalar Selvi G, Supriya Y, Gautam Srivastava, Praveen Kumar Reddy Maddikunta, Deepti Raj G, Rutvij H Jhaveri, Prabadevi B, Weizheng Wang, Athanasios V.Vasilakos, Thippa Reddy Gadekallu|[link](https://www.researchgate.net/publication/379857733_GPT_Generative_Pre-trained_Transformer_-_A_Comprehensive_Review_on_Enabling_Technologies_Potential_Applications_Emerging_Challenges_and_Future_Directions)|-|
|Mind the Gap: Understanding the Modality Gap in Multi-modal Contrastive Representation Learning|2022|Weixin Liang, Yuhui Zhang, Yongchan Kwon, Serena Yeung, James Zou|[link](https://arxiv.org/abs/2203.02053)|-|
|Maximum Mean Discrepancy Meets Neural Networks: The Radon-Kolmogorov-Smirnov Test|2022|Seunghoon Paik1, Michael Celentano1, Alden Green2, Ryan J. Tibshirani|[link](https://arxiv.org/pdf/2309.02422)|-|






